{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 0. 준비 ====\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env에서 OPENAI_API_KEY 로딩\n",
    "\n",
    "# ==== 1. LangChain 기본 ====\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # 또는 최신 gpt-4.1 / gpt-4o / gpt-4o-mini 등\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# LCEL: 프롬프트 → LLM → 출력\n",
    "chain = prompt | llm\n",
    "resp = chain.invoke({\"question\": \"LangChain이 뭐예요? 한 문단으로 설명해줘.\"})\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e71b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2. 미니 RAG(FAISS) ====\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 예시 문서\n",
    "docs = [\n",
    "    {\"id\": 1, \"text\": \"LangChain enables building LLM apps with components like prompts, chains, and agents.\"},\n",
    "    {\"id\": 2, \"text\": \"FAISS provides efficient vector similarity search for embeddings.\"},\n",
    "    {\"id\": 3, \"text\": \"RAG combines retrieval with generation to ground model answers in your data.\"},\n",
    "]\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = []\n",
    "for d in docs:\n",
    "    for c in splitter.split_text(d[\"text\"]):\n",
    "        texts.append(c)\n",
    "\n",
    "emb = OpenAIEmbeddings()\n",
    "vdb = FAISS.from_texts(texts, embedding=emb)\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    return vdb.similarity_search(query, k=k)\n",
    "\n",
    "# RAG 체인: 질의 → 검색 → 컨텍스트+질의 → 답변\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer based on CONTEXT.\\nCONTEXT:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"context\": lambda x: \"\\n\".join([d.page_content for d in retrieve(x[\"question\"])]),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(rag_chain.invoke({\"question\": \"What is RAG and why use FAISS?\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
